{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we have in diffusers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q diffusers\n",
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text2Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "def free_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StableDiffusionPipeline.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.__call__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.__call__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both base & refiner\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "base.enable_model_cpu_offload()\n",
    "\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "\n",
    "n_steps = 40\n",
    "high_noise_frac = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run base -> output latents (output_type=\"latent\")\n",
    "# run refiner -> pass image (latent from previous step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image2Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "\n",
    "\n",
    "pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\"\n",
    "init_image = load_image(url)\n",
    "\n",
    "display(init_image.resize((init_image.size[0] // 2, init_image.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n",
    "strength = 0.8\n",
    "assert 0 < strength < 1\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(2**24 + 43)\n",
    "image = pipeline(prompt, image=init_image, generator=generator, strength=strength).images[0]\n",
    "\n",
    "res = make_image_grid([init_image, image], rows=1, cols=2)\n",
    "display(res.resize((res.size[0] // 2, res.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(image):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = next(pipeline.vae.parameters()).dtype\n",
    "    \n",
    "    image = pipeline.image_processor.preprocess(image)\n",
    "    image = image.to(device=device, dtype=dtype)\n",
    "\n",
    "    if pipeline.vae.config.force_upcast:\n",
    "        image = image.float()\n",
    "        pipeline.vae.to(dtype=torch.float32)\n",
    "\n",
    "    latents = pipeline.vae.encode(image).latent_dist.sample()\n",
    "\n",
    "    if pipeline.vae.config.force_upcast:\n",
    "        pipeline.vae.to(dtype)\n",
    "\n",
    "    \n",
    "    latents = pipeline.vae.config.scaling_factor * latents\n",
    "    latents = latents.cpu()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return latents\n",
    "\n",
    "def normalize(tensor):\n",
    "    return (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "\n",
    "encoded = encode(init_image)\n",
    "encoded_image = to_pil_image(normalize(encoded)[0])\n",
    "display(encoded_image.resize((encoded_image.size[0] * 2, encoded_image.size[1] * 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def decode(encoded):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    encoded = encoded.to(device)\n",
    "    dtype = next(pipeline.vae.parameters()).dtype\n",
    "\n",
    "    if pipeline.vae.config.force_upcast:\n",
    "        encoded = encoded.float()\n",
    "        pipeline.vae.to(dtype=torch.float32)\n",
    "\n",
    "    decoded = pipeline.vae.decode(encoded / pipeline.vae.config.scaling_factor)\n",
    "\n",
    "    if pipeline.vae.config.force_upcast:\n",
    "        pipeline.vae.to(dtype)\n",
    "\n",
    "    decoded = decoded.sample.cpu()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return decoded\n",
    "\n",
    "decoded = decode(encoded)\n",
    "decoded_im = to_pil_image(normalize(decoded)[0])\n",
    "display(decoded_im.resize((decoded_im.size[0] // 2, decoded_im.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = make_image_grid([init_image, decoded_im], rows=1, cols=2)\n",
    "display(res.resize((res.size[0] // 2, res.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from transformers import pipeline\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\"\n",
    "prompt = \"An elegant cat with thick, fluffy black and white fur, sitting in a snowy winter landscape. Snowflakes gently fall around.\"\n",
    "original_image = load_image(\n",
    "    url\n",
    ")\n",
    "display(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    model_id, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canny version\n",
    "\n",
    "image = np.array(original_image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "display(canny_image.resize((canny_image.size[0] // 2, canny_image.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(int(10101))\n",
    "output = pipe(\n",
    "    prompt, image=canny_image, generator=generator\n",
    ").images[0]\n",
    "\n",
    "res = make_image_grid([original_image, canny_image, output], rows=1, cols=3)\n",
    "display(res.resize((res.size[0] // 2, res.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth\n",
    "depth_estimator = pipeline('depth-estimation')\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-depth\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    model_id, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = depth_estimator(original_image)['depth']\n",
    "image = np.array(image)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "depth_image = Image.fromarray(image)\n",
    "\n",
    "display(depth_image.resize((depth_image.size[0] // 2, depth_image.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(int(10101))\n",
    "output = pipe(\n",
    "    prompt, image=depth_image, generator=generator\n",
    ").images[0]\n",
    "\n",
    "res = make_image_grid([original_image, depth_image, output], rows=1, cols=3)\n",
    "display(res.resize((res.size[0] // 2, res.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IP-Adapter\n",
    "\n",
    "(this part is taken from diffusers docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "images = pipeline(\n",
    "    prompt=\"a polar bear sitting in a chair drinking a milkshake\",\n",
    "    ip_adapter_image=image,\n",
    "    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=100,\n",
    "    generator=generator,\n",
    ").images\n",
    "display(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can save embeds and use them as ip_adapter_image_embeds parameter\n",
    "image_embeds = pipeline.prepare_ip_adapter_image_embeds(\n",
    "    ip_adapter_image=image,\n",
    "    ip_adapter_image_embeds=None,\n",
    "    device=\"cuda\",\n",
    "    num_images_per_prompt=1,\n",
    "    do_classifier_free_guidance=True,\n",
    ")\n",
    "\n",
    "torch.save(image_embeds, \"image_embeds.ipadpt\")\n",
    "\n",
    "image_embeds = torch.load(\"image_embeds.ipadpt\")\n",
    "images = pipeline(\n",
    "    prompt=\"a polar bear sitting in a chair drinking a milkshake\",\n",
    "    ip_adapter_image_embeds=image_embeds,\n",
    "    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=100,\n",
    "    generator=generator,\n",
    ").images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.image_processor import IPAdapterMaskProcessor\n",
    "\n",
    "mask1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_mask1.png\")\n",
    "mask2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_mask2.png\")\n",
    "\n",
    "output_height = 1024\n",
    "output_width = 1024\n",
    "\n",
    "processor = IPAdapterMaskProcessor()\n",
    "masks = processor.preprocess([mask1, mask2], height=output_height, width=output_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=[\"ip-adapter-plus-face_sdxl_vit-h.safetensors\"])\n",
    "pipeline.set_ip_adapter_scale([[0.7, 0.7]])  # one scale for each image-mask pair\n",
    "\n",
    "face_image1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl1.png\")\n",
    "face_image2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl2.png\")\n",
    "\n",
    "ip_images = [[face_image1, face_image2]]\n",
    "\n",
    "masks = [masks.reshape(1, masks.shape[0], masks.shape[2], masks.shape[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "num_images = 1\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"2 girls\",\n",
    "    ip_adapter_image=ip_images,\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=20,\n",
    "    num_images_per_prompt=num_images,\n",
    "    generator=generator,\n",
    "    cross_attention_kwargs={\"ip_adapter_masks\": masks}\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi IP-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image, DDIMScheduler\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    image_encoder=image_encoder,\n",
    ")\n",
    "pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.load_ip_adapter(\n",
    "  \"h94/IP-Adapter\",\n",
    "  subfolder=\"sdxl_models\",\n",
    "  weight_name=[\"ip-adapter-plus_sdxl_vit-h.safetensors\", \"ip-adapter-plus-face_sdxl_vit-h.safetensors\"]\n",
    ")\n",
    "pipeline.set_ip_adapter_scale([0.7, 0.3])\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/women_input.png\")\n",
    "style_folder = \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy\"\n",
    "style_images = [load_image(f\"{style_folder}/img{i}.png\") for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"wonderwoman\",\n",
    "    ip_adapter_image=[style_images, face_image],\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=50, num_images_per_prompt=1,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ControlNet + IP-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "controlnet_model_path = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
    "\n",
    "pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16)\n",
    "pipeline.to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n",
    "\n",
    "ip_adapter_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/statue.png\")\n",
    "depth_map = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/depth.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "image = pipeline(\n",
    "    prompt=\"best quality, high quality\",\n",
    "    image=depth_map,\n",
    "    ip_adapter_image=ip_adapter_image,\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style-Layout control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "\n",
    "scale = {\n",
    "    \"down\": {\"block_2\": [0.0, 1.0]},\n",
    "    \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n",
    "}\n",
    "pipeline.set_ip_adapter_scale(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(26)\n",
    "image = pipeline(\n",
    "    prompt=\"a cat, masterpiece, best quality, high quality\",\n",
    "    ip_adapter_image=style_image,\n",
    "    negative_prompt=\"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n",
    "    guidance_scale=5,\n",
    "    num_inference_steps=30,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = {\n",
    "    \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n",
    "}\n",
    "pipeline.set_ip_adapter_scale(scale)\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(26)\n",
    "image = pipeline(\n",
    "    prompt=\"a cat, masterpiece, best quality, high quality\",\n",
    "    ip_adapter_image=style_image,\n",
    "    negative_prompt=\"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n",
    "    guidance_scale=5,\n",
    "    num_inference_steps=30,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
